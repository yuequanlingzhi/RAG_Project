{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "#pdf读取文本\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdfs(pdf_dir):\n",
    "    pdf_texts = {}\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            pdf_texts[filename] = extract_text_from_pdf(pdf_path)\n",
    "    return pdf_texts\n",
    "\n",
    "#文本截断为小chunk\n",
    "def split_into_chunks(text, max_chunk_size=512):\n",
    "    #以句号拆分\n",
    "    for char in ['\\n', '\\t', '.']:\n",
    "        text = text.replace(char, '')\n",
    "    sentences = text.split('。')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_chunk_size:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "pdf_dir = 'Final_Project_Documents/test'  #test只有一个较小pdf文件，测试用\n",
    "pdf_texts = extract_text_from_pdfs(pdf_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.datasets import SentencesDataset\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "#使用预训练模型all-mpnet-base-v2进行微调\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    for char in ['\\n', '\\t', '.']:\n",
    "        text = text.replace(char, '')\n",
    "    return text.split('。')\n",
    "\n",
    "\n",
    "sentences = []\n",
    "for doc in pdf_texts.keys():\n",
    "    sentences.extend(split_text_into_sentences(pdf_texts[doc]))\n",
    "\n",
    "\n",
    "train_examples = []\n",
    "for sentence in sentences:\n",
    "    train_examples.append(InputExample(texts=[sentence, sentence], label=1.0))\n",
    "train_data = SentencesDataset(train_examples, model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#微调all-mpnet-base-v2，采用无监督方法，但效果提升有限\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,  \n",
    "    warmup_steps=100,\n",
    ")\n",
    "\n",
    "model.save('fine_tuned_simcse_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 空气对物体表面产生压力的原因有两个：一个是上层空气的重量对下层空气造成了压力，在垂直方向上，越向上，大气压强就越低 另一个原因是空气分子不规则的热运动\n",
      "Score: 0.3539135456085205\n",
      "\n",
      "Chunk: 由于空气的黏性产生阻滞力一层一层的向外影响下去，就在机体表面形成了沿机体表面法向方向，流速由零逐渐增加到外界气流流速的薄薄的一层空气层，这就叫做附面层\n",
      "Score: 0.36287349462509155\n",
      "\n",
      "Chunk: 图1-4典型热气球结构热气球升力来源于球囊内热空气与环境空气的密度差，升力的大小与密度差成正比\n",
      "Score: 0.4387640357017517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 加载模型\n",
    "embedder = SentenceTransformer('all-mpnet-base-v2') #或fine_tuned_simcse_model\n",
    "\n",
    "# 创建FAISS索引\n",
    "def create_embeddings_and_index(pdf_texts, max_chunk_size=512):\n",
    "    all_chunks = []\n",
    "    for filename, text in pdf_texts.items():\n",
    "        chunks = split_into_chunks(text, max_chunk_size)\n",
    "        all_chunks.extend(chunks)  # 将所有chunk放入一个列表\n",
    "    \n",
    "    embeddings = embedder.encode(all_chunks, convert_to_numpy=True)\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return all_chunks, index\n",
    "\n",
    "all_chunks, index = create_embeddings_and_index(pdf_texts, max_chunk_size=128)\n",
    "\n",
    "# 搜索函数，检索相似的文段\n",
    "def search(query, all_chunks, index, k=3):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    return [(all_chunks[i], D[0][idx]) for idx, i in enumerate(I[0])]\n",
    "\n",
    "query = \"空气对物体表面产生压力的原因\"\n",
    "retrieved_chunks = search(query, all_chunks, index, k=3)\n",
    "for chunk, score in retrieved_chunks:\n",
    "    print(f\"Chunk: {chunk}\\nScore: {score}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: 旋翼航空器的定义\n",
      "相关信息:\n",
      "这是一种双模态的航空器，应视为飞机，也可将其作为有条件的特殊构型双旋翼航空器\n",
      "空气对物体表面产生压力的原因有两个：一个是上层空气的重量对下层空气造成了压力，在垂直方向上，越向上，大气压强就越低 另一个原因是空气分子不规则的热运动\n",
      "空气作用在与之有相对运动物体上的力称为空气动力 飞机飞行时，作用在飞机各部件上的空气动力的合力叫做飞机的总空气动力，用R表示\n",
      "答案: 主何经程度等着發现算節和品啊唯点系喜均基比类的限知道。\n",
      "更笑网: 安全都是那样的放没最大。 他们以世界的让线。 全自己的建跟确。 我从什么的话,但是付仅介任今的无法。 这美候增經的事。 邺于骑士的东边。 环塔拳疑的门间。 大提供場后,还是像次的人爱。 \"The first time I saw the world, I was a little bit scared. I thought that I would never see it again. But I did. It was the first day I ever saw it. And I didn't know what to do. The first thing I said to myself was, 'I'm going to go and see the thing.' And it was so beautiful. So beautiful that it made me feel like I had seen it before. That I could see that there was something there. There was nothing there, but I knew that something was there.\"\n",
      "首精神: 管理紅的是起来的。长接結的一格。嘴把突研需要质。虽然被持活虑身。白轻须撃联。您清普江暴的地方。令份仍价据的未村。 未柔本的发生。进街,涅娅。不是圣王国的感谢。是过去,队是期得到些色。你仰仙仮仏的迪術离。如演是慢业的我操。或者是不会落获。场近亚,战\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_name = \"gpt2\"  # gpt2表现很差，可以替换成其他生成模型，如\"t5-small\", \"gpt-3\"等，或者到时候直接在线调用某个现成的大模型，代码贴在这里可以应付检查\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 定义generator函数\n",
    "def generator(input_text, max_length=200, num_return_sequences=1):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,  # 防止重复n-gram\n",
    "        top_p=0.95,  # 使用nucleus采样\n",
    "        top_k=50,    # 限制采样的候选数量\n",
    "        temperature=0.1,  # 温度采样控制生成多样性\n",
    "        do_sample=True,  # 启用采样\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def generate_answer(query, retrieved_chunks):\n",
    "    context = \"\\n\".join([chunk for chunk, _ in retrieved_chunks])\n",
    "    input_text = f\"问题: {query}\\n相关信息:\\n{context}\\n答案:\"\n",
    "    response = generator(input_text, max_length=1024, num_return_sequences=1)\n",
    "    return response\n",
    "\n",
    "# 生成答案 ##这gpt2生成的是什么玩意？\n",
    "answer = generate_answer(query, retrieved_chunks)\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
