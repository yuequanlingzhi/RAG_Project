{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "#pdf读取文本\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdfs(pdf_dir):\n",
    "    pdf_texts = {}\n",
    "    for filename in os.listdir(pdf_dir):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_dir, filename)\n",
    "            pdf_texts[filename] = extract_text_from_pdf(pdf_path)\n",
    "    return pdf_texts\n",
    "\n",
    "#文本截断为小chunk\n",
    "def split_into_chunks(text, max_chunk_size=512):\n",
    "    #以句号拆分\n",
    "    for char in ['\\n', '\\t', '.']:\n",
    "        text = text.replace(char, '')\n",
    "    sentences = text.split('。')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_chunk_size:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "pdf_dir = r'Final_Project_Documents\\documents'  #test只有一个较小pdf文件，测试用\n",
    "pdf_texts = extract_text_from_pdfs(pdf_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练模型换成了multi-qa-MiniLM-L6-cos-v1，这个模型更小而且语义匹配的效果更好一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64064d0dbd244d0b9ed3a05a1af97bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Mypython\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\21102\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dde96059734da4af0a2546053b3010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd42e68f68084a5694a7a37eb0790741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a90ec8e31a4da3b52ce7b92fbb09ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c631eb09394724b7b70fd0da3d685b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c6aa590a7049e2aeded6100094376f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4162a8596f343358d9e209bb7802215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88d3524ed064c95b5054e47fcad8ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b5cff494c44959aeb9a8eda799b8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cca12bd92174f7a98716256a09c6206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0c52dbab5e42bc820d9c1443446164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 加载模型\n",
    "embedder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1') #或fine_tuned_simcse_model\n",
    "\n",
    "# 创建FAISS索引\n",
    "def create_embeddings_and_index(pdf_texts, max_chunk_size=512):\n",
    "    all_chunks = []\n",
    "    for filename, text in pdf_texts.items():\n",
    "        chunks = split_into_chunks(text, max_chunk_size)\n",
    "        all_chunks.extend(chunks)  # 将所有chunk放入一个列表\n",
    "    \n",
    "    embeddings = embedder.encode(all_chunks, convert_to_numpy=True)\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return all_chunks, index\n",
    "\n",
    "all_chunks, index = create_embeddings_and_index(pdf_texts, max_chunk_size=512)\n",
    "\n",
    "# 搜索函数，检索相似的文段\n",
    "def search(query, all_chunks, index, k=3):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_embedding, k)\n",
    "    return [(all_chunks[i], D[0][idx]) for idx, i in enumerate(I[0])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 定位器安装程序，通过导线和电缆的线号选择合适的夹接插钉/插孔，将定位器插入锁盘扭转90º， 取下夹接线号选择盘上的保险卡子， 选择合适的夹接档位到SELNO指示线上并到位，按压夹接工具的手柄操作夹接工具必须完成一个夹接循环，待夹接工具的模块到达夹接力矩时防倒转棘轮释放是夹接工具复位\n",
      "Score: 0.3770199716091156\n",
      "\n",
      "Chunk: 恢复配电板导线束的固定和捆扎；安装配电系图9-361 EN0313单相线路跳开关结构 统的配电板，摘下ESD防静电外带，摘下所有挂的红色警告牌，闭合线路跳开关，将驾驶舱所有操作过的电门放到正常位 给飞机提供电源对更换的线路跳开关系统进行操作测试，检验线路跳卡关的功能\n",
      "Score: 0.4064154028892517\n",
      "\n",
      "Chunk: (c) 对座舱和驾驶舱的下列构件进行检查：(1) 全面地检查是否整洁，是否有可能使控制机构失灵的外物和松动的设备；(2) 座椅和座椅安全带：检查是否处于不良状态和有明显缺陷；(3) 窗和风档：检查是否损坏和破裂；(4) 仪表：检查其状况、座架、标识是否符合要求，和检查其操作性能是否良好；(5) 飞行和发动机控制机构：检查是否安装正确、操作性能良好；(6) 电池：检查是否安装正确、充电恰当；(7) 各种系统：检查是否安装正确和有明显的缺陷，检查它们的一般状况\n",
      "Score: 0.46906566619873047\n",
      "\n",
      "Chunk: 将夹接工具的定位器安装在夹接工具上，按照插钉/插孔号选择20号钉位置（红色）并锁定位置， 按照AWG线号22线选择夹接工具的夹接档位  如果使用M22520/1-01夹接工具 （请见图8-387所示）或M22520/2-01夹接工具（请见图8-388所示） ，将插钉/插孔放入夹接工具的定位器，将绝缘去除完成的导线插入插钉/插孔的夹接筒， 按压夹接工具的手柄待到达夹接力矩时， 防倒转棘轮复位反转夹接工具手柄自动释放， 从夹接工具的定位器中取下夹接完成的插钉/插孔\n",
      "Score: 0.4961960017681122\n",
      "\n",
      "Chunk: 带安装边的轴有定位销孔，用螺栓和螺帽将螺旋桨固定在轴上 定位销孔让螺旋桨安装在一个位置 有的是预先将带螺纹的圈压入螺栓孔，不再需要螺帽（图11-44） 安装螺旋桨前,，先要检查凸缘有无锈蚀、缺口、毛刺和其它表面缺陷，带螺栓的孔和带螺纹的圈必须清洁并处于良好的状态；将螺旋桨安装到已准备好的发动机曲轴上，定位销应准确地落入定位孔内；安装螺栓、垫圈和螺帽，先轻轻地上紧所有的螺帽；用工具按规定的交错次序扭转螺帽到所要求的扭矩值；安装整流罩；进行轨迹调整；加上保险装置 图11-44带安装边的轴(a)带有定位销孔的安装边；(b)有螺套的安装边安装螺旋桨时，遵守螺旋桨制造厂建议的紧周次序是重要的，以避免在螺旋桨桨毂中导致应力（图11-45），多轮次按照规定顺序，交叉按力矩要求值拧紧螺栓后必须再依次检验各螺栓力矩值 民用航空器维修基础系列教材第5册 螺旋桨154图11-45紧固顺序在某些低马力发动机上，曲轴是锥形的，螺旋桨安装端带螺纹 为防止螺旋桨在轴上转动，在轴上装有一个大键槽，键保持螺旋桨在位(图11-46) 在安装螺旋桨之前，注意检查锥形轴上有无锈蚀、裂纹和磨损等缺陷\n",
      "Score: 0.5194090604782104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"定位器安装程序，通过导线和电缆的线号选择什么\"\n",
    "retrieved_chunks = search(query, all_chunks, index, k=5)\n",
    "for chunk, score in retrieved_chunks:\n",
    "    print(f\"Chunk: {chunk}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换成了gpt-neo-2.7B试了一下，不过这个模型参数量不少，显存消耗也很大（10-12GB）而且效果很大提升，我还是直接找了个api调用了。使用api的话下面的代码块可以忽略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"  # gpt2表现很差，可以替换成其他生成模型，如\"t5-small\", \"gpt-3\"等，或者到时候直接在线调用某个现成的大模型，代码贴在这里可以应付检查\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     model.to(device)\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "\n",
    "# 定义generator函数\n",
    "def generator(input_text, max_length=2048, num_return_sequences=1):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,  # 防止重复n-gram\n",
    "        top_p=0.95,  # 使用nucleus采样\n",
    "        top_k=50,    # 限制采样的候选数量\n",
    "        temperature=0.1,  # 温度采样控制生成多样性\n",
    "        do_sample=True,  # 启用采样\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def generate_answer(query, retrieved_chunks):\n",
    "    context = \"\\n\".join([chunk for chunk, _ in retrieved_chunks])\n",
    "    input_text = f\"问题: {query}\\n相关信息:\\n{context}\\n答案:\"\n",
    "    response = generator(input_text, max_length=2048, num_return_sequences=1)\n",
    "    return response\n",
    "\n",
    "# 生成答案 ##这gpt2生成的是什么玩意？\n",
    "# answer = generate_answer(query, retrieved_chunks)\n",
    "answer = generator(\"你好\",max_length=2048, num_return_sequences=1)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个key搞了个国内的镜像，不用梯子，我暂时配了10w字符的额度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "夹接插钉/插孔时，选择20号钉位置（红色），根据AWG线号22线选择夹接工具的夹接档位。"
     ]
    }
   ],
   "source": [
    "def generate_input_text(query, retrieved_chunks):\n",
    "    context = \"\\n\".join([chunk for chunk, _ in retrieved_chunks])\n",
    "    input_text = f\"问题: {query}\\n相关信息:\\n{context}\\n答案:\"\n",
    "    return input_text\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-fpifp5f293bak59ts7n71sohiqcnivi6qn9v9d08dcnhr9gp\",\n",
    "    base_url=\"https://api.aihao123.cn/luomacode-api/open-api/v1\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': generate_input_text(query, retrieved_chunks)},\n",
    "    ],\n",
    "    model='gpt-3.5-turbo',  # 代码提示上写了可以调用的模型\n",
    "    stream=True  # 一定要设置True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
